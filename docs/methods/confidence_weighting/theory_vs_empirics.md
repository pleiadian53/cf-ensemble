# Theory vs. Empirics: What Can Be Proven?

**Last Updated**: January 24, 2026

## Overview

This document clarifies what aspects of confidence weighting effectiveness can be **mathematically proven** versus what requires **empirical validation**.

---

## Summary Table

| Question | Can Prove? | Evidence Type | Status |
|----------|-----------|---------------|--------|
| Does confidence weighting help? | ‚ùå No | Empirical | ‚úÖ Verified (+1.7%) |
| Below some threshold, it doesn't help | ‚úÖ Yes* | Theoretical + Empirical | üîÑ Theory done, validating threshold |
| Above some threshold, minimal gains | ‚úÖ Yes | Theoretical (ceiling) | üîÑ Need empirical threshold |
| Diversity is necessary | ‚úÖ Yes | Theoretical (proof) | ‚úÖ Proven |
| Specific threshold values (60%, 80%) | ‚ùå No | Empirical only | ‚è≥ Need experiments |
| Strategy rankings by quality | ‚ùå No | Empirical | ‚è≥ Need experiments |
| Improvement magnitudes (+3-8%) | ‚ùå No | Empirical | ‚è≥ Need experiments |

*With assumptions (see below)

---

## What Can Be Proven

### 1. Information-Theoretic Lower Bound ‚úÖ

**Claim**: If classifiers are only slightly better than random, confidence weighting cannot help significantly.

**Proof Sketch**:

Let $p_{\text{correct}}$ be the probability a classifier is correct, and suppose $p_{\text{correct}} = 0.5 + \epsilon$ for small $\epsilon$.

The **mutual information** between confidence score $c$ and correctness $y$ is:
$$I(c; y) = H(y) - H(y | c)$$

Where:
- $H(y) \approx 1$ bit (for balanced classes)
- $H(y | c)$ is entropy of $y$ given confidence $c$

When classifiers are near-random ($\epsilon \approx 0$):
- Confidence scores weakly correlate with correctness
- $I(c; y) = O(\epsilon)$ bits

**Implication**: With $\epsilon < 0.1$ (i.e., accuracy < 60%), the confidence signal is too weak to exploit effectively.

**What we CANNOT prove**: The exact threshold (60% vs 55% vs 65%).

---

### 2. Ceiling Effect ‚úÖ

**Claim**: If baseline accuracy is already $1 - \delta$, maximum possible improvement is $\delta$.

**Proof**: Trivial.

Accuracy cannot exceed 100%. If baseline is 90%, maximum possible improvement is 10 percentage points.

In practice, irreducible error (Bayes error) means actual improvement $\ll \delta$.

**Implication**: At 85%+ accuracy, gains will be small (<5% realistically, <10% theoretically).

**What we CANNOT prove**: The exact quality level where returns become negligible (85% vs 80% vs 90%).

---

### 3. Diversity Necessity ‚úÖ

**Claim**: If all classifiers are identical, no weighting strategy can improve performance.

**Proof**:

Suppose all classifiers produce the same prediction: $r_{ui} = r_i$ for all $u$.

Any weighted ensemble prediction is:
$$\hat{y}_i = g\left(\sum_{u=1}^m w_u r_i\right) = g\left(r_i \sum_{u=1}^m w_u\right)$$

Since $\sum_{u} w_u$ is constant across instances, this is equivalent to $\hat{y}_i = g(r_i)$ for some function $g$.

Thus, all weighting schemes produce the same predictions ‚Üí no weighting can improve over uniform.

**Implication**: Diversity is **necessary** for confidence weighting to help.

**What we CANNOT prove**: How much diversity is sufficient, or how to quantify "enough" diversity.

---

### 4. Calibration-Strategy Interaction ‚úÖ

**Claim**: If confidence scores are **anti-calibrated** (high confidence ‚Üí low accuracy), certainty-based weighting **hurts** performance.

**Proof**:

Certainty strategy: $c_{ui} = |r_{ui} - 0.5|$ (weight by distance from 0.5).

If anti-calibrated:
- High confidence ($|r_{ui} - 0.5|$ large) ‚Üí Low accuracy
- Low confidence ($|r_{ui} - 0.5|$ small) ‚Üí High accuracy

Certainty strategy upweights high-confidence predictions, which are **systematically wrong** under anti-calibration.

Expected performance: **Worse** than uniform weighting.

**Empirical confirmation**: In our experiments, certainty strategy achieved **-1.3%** (worse than baseline) when classifiers had calibration issues.

**Implication**: Fixed strategies can hurt if assumptions violated. Learned reliability is more robust.

---

## What CANNOT Be Proven (Requires Empirics)

### 1. Specific Threshold Values ‚ùå

**Question**: Is the minimum viable quality 60% or 55% or 65%?

**Why unprovable**: Depends on:
- **Problem difficulty distribution**: Easy problems have lower thresholds
- **Classifier types**: Neural nets vs. decision trees have different calibration
- **Feature quality**: Better features ‚Üí better confidence signals even at lower accuracy
- **Dataset properties**: Size, noise level, class imbalance

**Need**: Systematic experiments across quality levels and datasets.

**Status**: 
- ‚úÖ Theory says "some threshold exists"
- ‚è≥ Experiments needed to determine actual value

---

### 2. Strategy Rankings ‚ùå

**Question**: Which strategy is best at which quality level?

**Why unprovable**: Strategy effectiveness depends on:
- Calibration quality (varies by classifier)
- Diversity patterns (varies by ensemble)
- Label availability (affects label-aware strategies)
- Instance difficulty distribution (varies by dataset)

**Need**: Quality √ó Strategy √ó Dataset grid search.

**Status**: 
- ‚úÖ Observed: Learned > Calibration > Certainty (at 73% quality in our data)
- ‚è≥ Need: Systematic variation to establish general patterns

---

### 3. Improvement Magnitudes ‚ùå

**Question**: How much improvement should we expect? +3-8%?

**Why unprovable**: Gain depends on:
- **Exploitable patterns**: How much do classifiers differ in their reliability profiles?
- **Quality-diversity interaction**: High diversity amplifies gains at moderate quality
- **Subgroup structure**: More complex subgroups ‚Üí larger potential gains

**Need**: Real-world datasets with known subgroup structures.

**Status**:
- ‚úÖ Observed: +1.7% at 73% quality (synthetic)
- ‚è≥ Expected: Larger gains on real biomedical data (more complex patterns)

---

### 4. Domain Generalization ‚ùå

**Question**: Do thresholds transfer across domains (vision ‚Üí NLP ‚Üí biomedical)?

**Why unprovable**: Different domains have:
- Different classifier calibration properties
- Different instance difficulty distributions  
- Different subgroup structures
- Different base classifier quality levels

**Need**: Multi-domain empirical study.

**Status**: ‚è≥ Not yet investigated

---

## Empirical Validation Plan

### Experiment 1: Quality Sweep ‚è≥

**Script**: `examples/quality_threshold_experiment.py`

**Design**:
- Vary quality: 50%, 55%, 60%, ..., 95%
- For each: Generate data, train all strategies, measure improvement
- 5 trials per quality level

**Will answer**:
- ‚úÖ Minimum viable quality (where improvement > 1%)
- ‚úÖ Peak improvement quality (sweet spot)
- ‚úÖ Diminishing returns threshold

**Expected result**: Inverted-U curve peaking at 70-80% quality.

**Status**: ‚úÖ Script ready, ‚è≥ need to run

---

### Experiment 2: Quality √ó Diversity Grid ‚è≥

**Design**:
```python
qualities = [0.60, 0.70, 0.80]
diversities = ['low', 'medium', 'high']
# 3 √ó 3 = 9 conditions
```

**Will answer**:
- ‚úÖ Does diversity amplify gains?
- ‚úÖ Is diversity more important at certain quality levels?

**Expected result**: Diversity effect strongest at moderate quality (70%).

**Status**: ‚è≥ Not yet implemented

---

### Experiment 3: Real Biomedical Datasets üéØ

**Datasets**:
1. Gene expression classification (multiple tissue types)
2. Clinical text analysis (ICD code prediction)
3. Medical image ensembles (radiology diagnosis)

**Will answer**:
- ‚úÖ Do thresholds hold on real data?
- ‚úÖ Are gains larger than synthetic (+5-12% hypothesized)?
- ‚úÖ Domain-specific variations?

**Status**: ‚è≥ Need access to datasets

---

## Current Evidence Status

### What We Know (Verified) ‚úÖ

| Finding | Evidence | Confidence |
|---------|----------|-----------|
| Confidence weighting **can** improve | Observed +1.7% | **High** ‚úÖ |
| Some strategies **hurt** if miscalibrated | Observed -1.3% (certainty) | **High** ‚úÖ |
| Learned reliability > Fixed strategies | Consistent across runs | **High** ‚úÖ |
| Diversity is necessary | Theoretical proof + observation | **Very High** ‚úÖ |
| There exists a lower threshold | Information theory | **High** ‚úÖ |
| There exists an upper threshold | Ceiling effect (math) | **Very High** ‚úÖ |

### What We Think (Hypothesized) üîÑ

| Hypothesis | Confidence | Next Step |
|-----------|-----------|-----------|
| 60% minimum viable quality | **Medium** | Run Experiment 1 |
| 65-80% sweet spot | **Medium** | Run Experiment 1 |
| >85% diminishing returns | **Medium-High** | Run Experiment 1 |
| Diversity amplifies gains | **Medium** | Run Experiment 2 |
| +3-8% at sweet spot | **Low-Medium** | Real data experiments |
| Larger gains on real data | **Medium** | Biomedical datasets |

### What We Don't Know ‚ùì

- Exact threshold values for different domains
- Strategy rankings at each quality level
- Interaction with other hyperparameters (œÅ, Œª, d)
- Multi-class classification thresholds
- Active learning integration effects

---

## Recommendations

### For Documentation

1. **Be explicit about evidence status**:
   - ‚úÖ Proven theoretically
   - ‚úÖ Verified empirically
   - üîÑ Hypothesized (being validated)
   - ‚ùì Unknown

2. **Update claims as experiments complete**:
   - After Experiment 1: Update threshold values
   - After Experiment 2: Update diversity effects
   - After Experiment 3: Add domain-specific guidance

3. **Acknowledge limitations**:
   - Synthetic data may not reflect real-world complexity
   - Thresholds may vary by domain
   - Guidelines are starting points, not guarantees

### For Users

**Current best practice**:

1. **Before using confidence weighting**:
   ```python
   from cfensemble.utils import diagnose_ensemble_quality
   
   recommendation = diagnose_ensemble_quality(R, labels, labeled_idx)
   print_diagnosis(recommendation)
   ```

2. **Interpret recommendations as guidelines**:
   - If diagnosis says "POOR" (<60%) ‚Üí Fix classifiers **likely** better than weighting
   - If diagnosis says "OPTIMAL" (60-85%) ‚Üí Weighting **likely** helps
   - If diagnosis says "EXCELLENT" (>85%) ‚Üí Weighting **likely** minimal impact

3. **Always validate empirically** on your data:
   - Try multiple strategies
   - Use cross-validation
   - Don't assume thresholds transfer exactly

### For Researchers

**Open questions** (publication opportunities):

1. **Theoretical**: Can we derive tighter bounds on improvement as a function of quality and diversity?

2. **Empirical**: Do thresholds generalize across domains (vision, NLP, biomedical, tabular)?

3. **Methodological**: Can we predict improvement **before** training (diagnostic tool)?

4. **Extensions**: How do thresholds change for:
   - Multi-class classification?
   - Imbalanced datasets?
   - Online/streaming data?
   - Non-IID data?

---

## Conclusion

### The 80/20 Rule

**80% is theory + informed reasoning**:
- ‚úÖ Some lower threshold exists (proven)
- ‚úÖ Some upper threshold exists (proven)
- ‚úÖ Diversity is necessary (proven)
- ‚úÖ Fixed strategies can hurt (observed)

**20% is specific numbers**:
- ‚è≥ 60% vs 55% vs 65% minimum (needs experiments)
- ‚è≥ 70-80% vs 65-75% sweet spot (needs experiments)
- ‚è≥ +3-8% vs +2-5% expected gain (needs experiments)

### Honest Summary

**What we can say with confidence**:
> "Confidence weighting effectiveness depends on base classifier quality. There exists a minimum quality below which it doesn't help (information-theoretic), and an upper quality above which gains are minimal (ceiling effect). Our initial experiments suggest a minimum around 60% accuracy and peak gains at 70-80%, but systematic validation is needed to confirm these specific thresholds."

**What we should NOT claim yet**:
> ~~"Confidence weighting requires 60% minimum accuracy."~~ (Too specific without validation)

**Better framing**:
> "Based on initial experiments and theory, we hypothesize a minimum viable quality around 60% accuracy. Experiment 1 will validate this threshold systematically."

---

**Status**: Living document, updated as experiments complete.  
**Next Update**: After running `quality_threshold_experiment.py`  
**Contributors**: CF-Ensemble Development Team
